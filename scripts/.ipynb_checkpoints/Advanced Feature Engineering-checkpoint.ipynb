{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec98a6f3-08e5-4fa7-a163-79f756b7cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7717fa36-42c5-4b8e-b46a-e8397f953bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/electrify__applied_ai_engineer__training_data.csv\"\n",
    "df = pd.read_csv(file_path) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf49624f-ba73-4750-abda-c7abacbb4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_no_stop(text):\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", str(text).lower())\n",
    "    return [w for w in tokens if (w not in ENGLISH_STOP_WORDS) & (len(w) >= 2) & (not(w.isdigit()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f27b42-f2c7-4d9f-b35f-97cf2c80df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['title'].apply(tokenize_no_stop)\n",
    "rez = {}\n",
    "for channel_id, group in df.groupby('channel_id'):\n",
    "    threshold = group['views_in_period'].quantile(0.75)\n",
    "    high_group = group[group['views_in_period'] >= threshold]\n",
    "    low_group = group[group['views_in_period'] < threshold]\n",
    "    high_counts = Counter()\n",
    "    for tokens in high_group['tokens']:\n",
    "        high_counts.update(set(tokens))\n",
    "    low_counts = Counter()\n",
    "    for tokens in low_group['tokens']:\n",
    "        low_counts.update(set(tokens))\n",
    "    n_high = len(high_group)\n",
    "    n_low = len(low_group)\n",
    "    keyword_scores = {}\n",
    "    for token in set(high_counts.keys()).union(low_counts.keys()):\n",
    "        high_freq = high_counts[token] / n_high if n_high else 0\n",
    "        low_freq = low_counts[token] / n_low if n_low else 0\n",
    "        score = (high_freq + 1e-5) / (low_freq + 1e-5)\n",
    "        keyword_scores[token] = score\n",
    "    sorted_tokens = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_keywords = [t for t,_ in sorted_tokens[:20]]\n",
    "    low_keywords = [t for t,_ in sorted_tokens[-20:]]\n",
    "    rez[channel_id] = {'top_keywords': top_keywords, 'low_keywords': low_keywords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bef3752-459b-43e9-9f22-1d8d68b5deb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UC-9b7aDP6ZN0coj9-xFnrtw': {'top_keywords': ['voyager',\n",
       "   'outside',\n",
       "   'experiments',\n",
       "   'eyes',\n",
       "   'journey',\n",
       "   'reach',\n",
       "   'shift',\n",
       "   'impact',\n",
       "   'detected',\n",
       "   'deepest',\n",
       "   'wrong',\n",
       "   'asteroid',\n",
       "   'exoplanet',\n",
       "   'probes',\n",
       "   'seen',\n",
       "   'facts',\n",
       "   'backwards',\n",
       "   'dinosaurs',\n",
       "   'discover',\n",
       "   'experiment'],\n",
       "  'low_keywords': ['designed',\n",
       "   'universe',\n",
       "   'satellites',\n",
       "   'crash',\n",
       "   'scientists',\n",
       "   'theory',\n",
       "   'star',\n",
       "   'black',\n",
       "   'model',\n",
       "   'big',\n",
       "   'weird',\n",
       "   'space',\n",
       "   'physics',\n",
       "   'explain',\n",
       "   'mercury',\n",
       "   'shouldn',\n",
       "   'nasa',\n",
       "   'images',\n",
       "   'years',\n",
       "   'moon']},\n",
       " 'UC510QYlOlKNyhy_zdQxnGYw': {'top_keywords': ['notable',\n",
       "   'stood',\n",
       "   'screwups',\n",
       "   'horrors',\n",
       "   'man',\n",
       "   'invade',\n",
       "   'castle',\n",
       "   'overpowered',\n",
       "   'fearsome',\n",
       "   'panther',\n",
       "   'strv',\n",
       "   'lifestyle',\n",
       "   'stop',\n",
       "   'saddam',\n",
       "   'evil',\n",
       "   'luxury',\n",
       "   'oversized',\n",
       "   'forget',\n",
       "   'lai',\n",
       "   'flattest'],\n",
       "  'low_keywords': ['weird',\n",
       "   'items',\n",
       "   'shoot',\n",
       "   'know',\n",
       "   'don',\n",
       "   'terrifying',\n",
       "   'youtube',\n",
       "   'pov',\n",
       "   'vietnam',\n",
       "   'tactics',\n",
       "   'tanks',\n",
       "   'ww1',\n",
       "   'things',\n",
       "   'wwii',\n",
       "   'story',\n",
       "   'north',\n",
       "   'gun',\n",
       "   'vs',\n",
       "   'soldier',\n",
       "   'korean']},\n",
       " 'UC_WXkNIJ2ncLIsAk_ltbvjA': {'top_keywords': ['remember',\n",
       "   'fakes',\n",
       "   'extreme',\n",
       "   'gets',\n",
       "   'wants',\n",
       "   'apart',\n",
       "   'destroyed',\n",
       "   'lazy',\n",
       "   'minifigures',\n",
       "   'play',\n",
       "   'features',\n",
       "   'rules',\n",
       "   'break',\n",
       "   'sets',\n",
       "   'bricks',\n",
       "   'worst',\n",
       "   'iq',\n",
       "   'lego',\n",
       "   'crazy',\n",
       "   'like'],\n",
       "  'low_keywords': ['hottest',\n",
       "   'conspiracy',\n",
       "   'far',\n",
       "   'tiny',\n",
       "   'cars',\n",
       "   'lots',\n",
       "   'normal',\n",
       "   'gone',\n",
       "   'hiding',\n",
       "   'theories',\n",
       "   'awkward',\n",
       "   '100x',\n",
       "   'techniques',\n",
       "   'set',\n",
       "   'building',\n",
       "   'vs',\n",
       "   'obsessions',\n",
       "   'takes',\n",
       "   'weird',\n",
       "   'hot']}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bbe3335-55e1-4662-a2e5-e957b9219df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_rez = {}\n",
    "for channel_id, group in df.groupby(\"channel_id\"):\n",
    "    threshold = group[\"views_in_period\"].quantile(0.75)\n",
    "    high_group = group[group[\"views_in_period\"] >= threshold]\n",
    "    low_group  = group[group[\"views_in_period\"] <  threshold]\n",
    "\n",
    "    vectoriser = TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        token_pattern=r\"\\b[a-zA-Z]{2,}\\b\",\n",
    "    )\n",
    "    titles_all = pd.concat([high_group[\"title\"], low_group[\"title\"]])\n",
    "    vectoriser.fit(titles_all)\n",
    "\n",
    "    tfidf_high = vectoriser.transform(high_group[\"title\"])\n",
    "    tfidf_low  = vectoriser.transform(low_group[\"title\"])\n",
    "    vocab = vectoriser.get_feature_names_out()\n",
    "\n",
    "    high_avg = tfidf_high.sum(axis=0).A1 / max(len(high_group), 1)\n",
    "    low_avg  = tfidf_low.sum(axis=0).A1  / max(len(low_group), 1)\n",
    "\n",
    "    tfidf_scores = {\n",
    "        token: (high_avg[idx] + 1e-6) / (low_avg[idx] + 1e-6)\n",
    "        for idx, token in enumerate(vocab)\n",
    "    }\n",
    "\n",
    "    sorted_tfidf = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    tfidf_top = [t for t, _ in sorted_tfidf[:10]]\n",
    "    tfidf_low = [t for t, _ in sorted_tfidf[-10:]]\n",
    "\n",
    "    tfidf_rez[channel_id] = {\n",
    "        \"tfidf_top_keywords\": tfidf_top,\n",
    "        \"tfidf_low_keywords\": tfidf_low,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b415720-c64e-4fe1-a8cf-c2a5c4cd4aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UC-9b7aDP6ZN0coj9-xFnrtw': {'tfidf_top_keywords': ['voyager',\n",
       "   'aliens',\n",
       "   'discover',\n",
       "   'detected',\n",
       "   'edge',\n",
       "   'deepest',\n",
       "   'sdo',\n",
       "   'seen',\n",
       "   'jupiter',\n",
       "   'neptune'],\n",
       "  'tfidf_low_keywords': ['black',\n",
       "   'big',\n",
       "   'years',\n",
       "   'weird',\n",
       "   'mercury',\n",
       "   'images',\n",
       "   'space',\n",
       "   'nasa',\n",
       "   'satellites',\n",
       "   'moon']},\n",
       " 'UC510QYlOlKNyhy_zdQxnGYw': {'tfidf_top_keywords': ['oversized',\n",
       "   'best',\n",
       "   'disappointments',\n",
       "   'betrayals',\n",
       "   'unfair',\n",
       "   'today',\n",
       "   'assassinations',\n",
       "   'fearsome',\n",
       "   'notable',\n",
       "   'warriors'],\n",
       "  'tfidf_low_keywords': ['vs',\n",
       "   'story',\n",
       "   'terrifying',\n",
       "   'axis',\n",
       "   'wwii',\n",
       "   'north',\n",
       "   'korean',\n",
       "   'tanks',\n",
       "   'things',\n",
       "   'soldier']},\n",
       " 'UC_WXkNIJ2ncLIsAk_ltbvjA': {'tfidf_top_keywords': ['apart',\n",
       "   'remember',\n",
       "   'extreme',\n",
       "   'fakes',\n",
       "   'gets',\n",
       "   'lazy',\n",
       "   'destroyed',\n",
       "   'wants',\n",
       "   'minifigures',\n",
       "   'features'],\n",
       "  'tfidf_low_keywords': ['obsession',\n",
       "   'talks',\n",
       "   'set',\n",
       "   'vs',\n",
       "   'building',\n",
       "   'techniques',\n",
       "   'obsessions',\n",
       "   'weird',\n",
       "   'hot',\n",
       "   'takes']}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_rez"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
